---
title: 中文分词算法介绍
author: Scott
tags:
  - 算法
categories: []
date: 2019-07-08 14:38:00
---

## 背景
* 英文以空格作为分隔符，而中文词语之间没有分隔；
* 在中文里，“词”和“词组”边界模糊现代汉语的基本表达单元虽然为“词”，且以双字或者多字词居多，但由于人们认识水平的不同，对词和短语的边界很难去区分；

中文分词的方法其实不局限于中文应用，也被应用到英文处理，如手写识别，单词之间的空格就不很清楚，中文分词方法可以帮助判别英文单词的边界。

## 分词算法
现有的分词算法可以分为三大类：

* 基于字符串匹配的分词方法
* 基于理解的分词方法
* 基于统计以及机器学习的分词方法

### 基于字符串匹配的分词方法
这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；常用的几种机械分词方法如下：

* 正向最大匹配法（由左到右的方向）；
* 逆向最大匹配法（由右到左的方向）；
* 最少切分（使每一句中切出的词数最小）；
* 双向最大匹配法（进行由左到右、由右到左两次扫描）

还可以将上述各种方法相互组合。例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。

统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。

### 基于理解的分词方法
这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。

### 基于统计以及机器学习的分词方法
这类分词基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据（标注好的语料）对模型参数进行估计，即训练。 在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。常见的序列标注模型有HMM和CRF。

从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。

但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如“这一”、“之一”、“有的”、“我的”、“许多的”等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。

另外一类是基于统计机器学习的方法。首先给出大量已经分词的文本，利用统计机器学习模型学习词语切分的规律（称为训练），从而实现对未知文本的切分。我们知道，汉语中各个字单独作词语的能力是不同的，此外有的字常常作为前缀出现，有的字却常常作为后缀（“者”“性”），结合两个字相临时是否成词的信息，这样就得到了许多与分词有关的知识。这种方法就是充分利用汉语组词的规律来分词。

优点：

* 这类分词算法能很好处理歧义和未登录词问题，效果比前一类效果好；

缺点：

* 需要预先有大量人工标注数据分好词的语料作支撑；
* 较慢的分词速度；
* 训练过程中时空开销极大。

### 分词方法总结
到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。例如，海量科技的分词算法就采用“复方分词法”，所谓复方，就是像中西医结合般综合运用机械方法和知识方法。对于成熟的中文分词系统，需要多种算法综合处理问题。

## 常用的分词工具
### 基于字符串匹配的分词工具

* 在github开源的 [ik-analyzer](https://code.google.com/archive/p/ik-analyzer/)，项目源码：https://github.com/wks/ik-analyzer
* 用Java开发的 [Paoding 分词器](https://code.google.com/archive/p/paoding/) ，[基于Lucene4.x的paoding分词器](http://git.oschina.net/zhzhenqin/paoding-analysis) 源码；

### 基于统计的分词工具

* 中科院的 [ICTCLAS](http://ictclas.nlpir.org/docs) 分词，以及Java版的实现 [ansj](https://github.com/NLPchina/ansj_seg)，ansj 的性能已超越 [ICTCLAS](http://ictclas.nlpir.org/docs) ；ICTCLAS是基于HMM的分词库。


## 参考资料
[有哪些比较好的中文分词方案？](https://www.zhihu.com/question/19578687)
[中文分词-百度百科](http://baike.baidu.com/view/19109.htm)
[常用的开源中文分词工具 ](http://www.scholat.com/vpost.html?pid=4477)
[NLP中的中文分词技术](http://www.open-open.com/lib/view/open1420814197171.html)